PS E:\ANU\S1\6528-Computer Vision\Mini Project\code> & D:/anaconda3/envs/comp6528/python.exe "e:/ANU/S1/6528-Computer Vision/Mini Project/code/train.py"
Using device: cuda
e:\ANU\S1\6528-Computer Vision\Mini Project\code\dataset\single_image_dataset.py:18: UserWarning: Argument(s) 'alpha_affine' are not valid for transform ElasticTransform
  A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0),
Epoch [1/10] Batch [0/100] D_loss: 0.5093, G_loss: 37.4945, L1: 36.5203
Epoch [1/10] Batch [10/100] D_loss: 0.2841, G_loss: 27.8236, L1: 27.2302
Epoch [1/10] Batch [20/100] D_loss: 0.2286, G_loss: 19.7908, L1: 19.3548
Epoch [1/10] Batch [30/100] D_loss: 0.2274, G_loss: 14.1779, L1: 13.7976
Epoch [1/10] Batch [40/100] D_loss: 0.2344, G_loss: 11.0632, L1: 10.7092
Epoch [1/10] Batch [50/100] D_loss: 0.2386, G_loss: 9.4174, L1: 9.0811
Epoch [1/10] Batch [60/100] D_loss: 0.2400, G_loss: 8.5657, L1: 8.2421
Epoch [1/10] Batch [70/100] D_loss: 0.2387, G_loss: 8.1276, L1: 7.8153
Epoch [1/10] Batch [80/100] D_loss: 0.2372, G_loss: 7.8365, L1: 7.5357
Epoch [1/10] Batch [90/100] D_loss: 0.2373, G_loss: 7.6147, L1: 7.3231
Epoch [2/10] Batch [0/100] D_loss: 0.2381, G_loss: 7.4192, L1: 7.1337
Epoch [2/10] Batch [10/100] D_loss: 0.2392, G_loss: 7.2360, L1: 6.9549
Epoch [2/10] Batch [20/100] D_loss: 0.2412, G_loss: 7.0736, L1: 6.7906
Epoch [2/10] Batch [30/100] D_loss: 0.2430, G_loss: 6.9162, L1: 6.6425
Epoch [2/10] Batch [40/100] D_loss: 0.2444, G_loss: 6.7789, L1: 6.5049
Epoch [2/10] Batch [50/100] D_loss: 0.2454, G_loss: 6.6392, L1: 6.3618
Epoch [2/10] Batch [60/100] D_loss: 0.2457, G_loss: 6.5010, L1: 6.2298
Epoch [2/10] Batch [70/100] D_loss: 0.2457, G_loss: 6.3859, L1: 6.1119
Epoch [2/10] Batch [80/100] D_loss: 0.2455, G_loss: 6.2724, L1: 5.9988
Epoch [2/10] Batch [90/100] D_loss: 0.2451, G_loss: 6.1618, L1: 5.8903
Epoch [3/10] Batch [0/100] D_loss: 0.2451, G_loss: 6.0683, L1: 5.7860
Epoch [3/10] Batch [10/100] D_loss: 0.2444, G_loss: 5.9563, L1: 5.6857
Epoch [3/10] Batch [20/100] D_loss: 0.2440, G_loss: 5.8655, L1: 5.5890
Epoch [3/10] Batch [30/100] D_loss: 0.2437, G_loss: 5.7718, L1: 5.4964
Epoch [3/10] Batch [40/100] D_loss: 0.2430, G_loss: 5.6800, L1: 5.4075
Epoch [3/10] Batch [50/100] D_loss: 0.2432, G_loss: 5.6098, L1: 5.3223
Epoch [3/10] Batch [60/100] D_loss: 0.2420, G_loss: 5.5126, L1: 5.2407
Epoch [3/10] Batch [70/100] D_loss: 0.2420, G_loss: 5.4483, L1: 5.1619
Epoch [3/10] Batch [80/100] D_loss: 0.2411, G_loss: 5.3586, L1: 5.0852
Epoch [3/10] Batch [90/100] D_loss: 0.2408, G_loss: 5.2865, L1: 5.0060
Epoch [4/10] Batch [0/100] D_loss: 0.2403, G_loss: 5.2054, L1: 4.9329
Epoch [4/10] Batch [10/100] D_loss: 0.2399, G_loss: 5.1436, L1: 4.8625
Epoch [4/10] Batch [20/100] D_loss: 0.2395, G_loss: 5.0695, L1: 4.7942
Epoch [4/10] Batch [30/100] D_loss: 0.2393, G_loss: 5.0133, L1: 4.7279
Epoch [4/10] Batch [40/100] D_loss: 0.2389, G_loss: 4.9469, L1: 4.6635
Epoch [4/10] Batch [50/100] D_loss: 0.2386, G_loss: 4.8854, L1: 4.6004
Epoch [4/10] Batch [60/100] D_loss: 0.2382, G_loss: 4.8254, L1: 4.5398
Epoch [4/10] Batch [70/100] D_loss: 0.2378, G_loss: 4.7666, L1: 4.4808
Epoch [4/10] Batch [80/100] D_loss: 0.2374, G_loss: 4.7107, L1: 4.4243
Epoch [4/10] Batch [90/100] D_loss: 0.2377, G_loss: 4.6697, L1: 4.3693
Epoch [5/10] Batch [0/100] D_loss: 0.2359, G_loss: 4.5853, L1: 4.3154
Epoch [5/10] Batch [10/100] D_loss: 0.2369, G_loss: 4.5117, L1: 4.2649
Epoch [5/10] Batch [20/100] D_loss: 0.2355, G_loss: 4.4799, L1: 4.2196
Epoch [5/10] Batch [30/100] D_loss: 0.2354, G_loss: 4.4283, L1: 4.1686
Epoch [5/10] Batch [40/100] D_loss: 0.2352, G_loss: 4.3819, L1: 4.1234
Epoch [5/10] Batch [50/100] D_loss: 0.2349, G_loss: 4.3391, L1: 4.0807
Epoch [5/10] Batch [60/100] D_loss: 0.2346, G_loss: 4.3002, L1: 4.0422
Epoch [5/10] Batch [70/100] D_loss: 0.2342, G_loss: 4.2602, L1: 4.0000
Epoch [5/10] Batch [80/100] D_loss: 0.2340, G_loss: 4.2206, L1: 3.9610
Epoch [5/10] Batch [90/100] D_loss: 0.2338, G_loss: 4.1838, L1: 3.9247
Epoch [6/10] Batch [0/100] D_loss: 0.2335, G_loss: 4.1500, L1: 3.8910
Epoch [6/10] Batch [10/100] D_loss: 0.2332, G_loss: 4.1162, L1: 3.8563
Epoch [6/10] Batch [20/100] D_loss: 0.2328, G_loss: 4.0825, L1: 3.8216
Epoch [6/10] Batch [30/100] D_loss: 0.2326, G_loss: 4.0512, L1: 3.7904
Epoch [6/10] Batch [40/100] D_loss: 0.2324, G_loss: 4.0213, L1: 3.7606
Epoch [6/10] Batch [50/100] D_loss: 0.2322, G_loss: 3.9917, L1: 3.7313
Epoch [6/10] Batch [60/100] D_loss: 0.2319, G_loss: 3.9629, L1: 3.7023
Epoch [6/10] Batch [70/100] D_loss: 0.2315, G_loss: 3.9349, L1: 3.6729
Epoch [6/10] Batch [80/100] D_loss: 0.2313, G_loss: 3.9070, L1: 3.6446
Epoch [6/10] Batch [90/100] D_loss: 0.2310, G_loss: 3.8804, L1: 3.6175
Epoch [7/10] Batch [0/100] D_loss: 0.2307, G_loss: 3.8552, L1: 3.5915
Epoch [7/10] Batch [10/100] D_loss: 0.2305, G_loss: 3.8290, L1: 3.5661
Epoch [7/10] Batch [20/100] D_loss: 0.2303, G_loss: 3.8036, L1: 3.5396
Epoch [7/10] Batch [30/100] D_loss: 0.2302, G_loss: 3.7887, L1: 3.5223
Epoch [7/10] Batch [40/100] D_loss: 0.2301, G_loss: 3.7514, L1: 3.4892
Epoch [7/10] Batch [50/100] D_loss: 0.2300, G_loss: 3.7277, L1: 3.4643
Epoch [7/10] Batch [60/100] D_loss: 0.2298, G_loss: 3.7041, L1: 3.4403
Epoch [7/10] Batch [70/100] D_loss: 0.2296, G_loss: 3.6811, L1: 3.4173
Epoch [7/10] Batch [80/100] D_loss: 0.2294, G_loss: 3.6583, L1: 3.3950
Epoch [7/10] Batch [90/100] D_loss: 0.2293, G_loss: 3.6416, L1: 3.3807
Epoch [8/10] Batch [0/100] D_loss: 0.2287, G_loss: 3.6149, L1: 3.3497
Epoch [8/10] Batch [10/100] D_loss: 0.2286, G_loss: 3.5922, L1: 3.3269
Epoch [8/10] Batch [20/100] D_loss: 0.2284, G_loss: 3.5713, L1: 3.3061
Epoch [8/10] Batch [30/100] D_loss: 0.2282, G_loss: 3.5514, L1: 3.2864
Epoch [8/10] Batch [40/100] D_loss: 0.2280, G_loss: 3.5322, L1: 3.2661
Epoch [8/10] Batch [50/100] D_loss: 0.2279, G_loss: 3.5113, L1: 3.2447
Epoch [8/10] Batch [60/100] D_loss: 0.2278, G_loss: 3.4920, L1: 3.2250
Epoch [8/10] Batch [70/100] D_loss: 0.2278, G_loss: 3.4803, L1: 3.2113
Epoch [8/10] Batch [80/100] D_loss: 0.2276, G_loss: 3.4607, L1: 3.1894
Epoch [8/10] Batch [90/100] D_loss: 0.2284, G_loss: 3.4416, L1: 3.1745
Epoch [9/10] Batch [0/100] D_loss: 0.2275, G_loss: 3.4561, L1: 3.1650
Epoch [9/10] Batch [10/100] D_loss: 0.2297, G_loss: 3.4558, L1: 3.1294
Epoch [9/10] Batch [20/100] D_loss: 0.2262, G_loss: 3.4295, L1: 3.1119
Epoch [9/10] Batch [30/100] D_loss: 0.2250, G_loss: 3.4182, L1: 3.1048
Epoch [9/10] Batch [40/100] D_loss: 0.2252, G_loss: 3.3883, L1: 3.0791
Epoch [9/10] Batch [50/100] D_loss: 0.2275, G_loss: 3.4036, L1: 3.0652
Epoch [9/10] Batch [60/100] D_loss: 0.2257, G_loss: 3.3708, L1: 3.0444
Epoch [9/10] Batch [70/100] D_loss: 0.2254, G_loss: 3.3516, L1: 3.0290
Epoch [9/10] Batch [80/100] D_loss: 0.2258, G_loss: 3.3410, L1: 3.0114
Epoch [9/10] Batch [90/100] D_loss: 0.2251, G_loss: 3.3191, L1: 2.9954
Epoch [10/10] Batch [0/100] D_loss: 0.2253, G_loss: 3.3071, L1: 2.9794
Epoch [10/10] Batch [10/100] D_loss: 0.2249, G_loss: 3.2892, L1: 2.9635
Epoch [10/10] Batch [20/100] D_loss: 0.2249, G_loss: 3.2765, L1: 2.9480
Epoch [10/10] Batch [30/100] D_loss: 0.2246, G_loss: 3.2588, L1: 2.9322
Epoch [10/10] Batch [40/100] D_loss: 0.2247, G_loss: 3.2464, L1: 2.9167
Epoch [10/10] Batch [50/100] D_loss: 0.2245, G_loss: 3.2304, L1: 2.9015
Epoch [10/10] Batch [60/100] D_loss: 0.2244, G_loss: 3.2172, L1: 2.8871
Epoch [10/10] Batch [70/100] D_loss: 0.2243, G_loss: 3.2030, L1: 2.8724
Epoch [10/10] Batch [80/100] D_loss: 0.2242, G_loss: 3.1888, L1: 2.8580
Epoch [10/10] Batch [90/100] D_loss: 0.2241, G_loss: 3.1747, L1: 2.8435
Training finished. Models saved.