PS E:\ANU\S1\6528-Computer Vision\Mini Project\code> & D:/anaconda3/envs/comp6528/python.exe "e:/ANU/S1/6528-Computer Vision/Mini Project/code/train.py"
Using device: cuda
e:\ANU\S1\6528-Computer Vision\Mini Project\code\dataset\single_image_dataset.py:18: UserWarning: Argument(s) 'alpha_affine' are not valid for transform ElasticTransform
  A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0),
Epoch [1/10] Batch [0/100] D_loss: 0.4898, G_loss: 60.9938, L1: 60.0686
Epoch [1/10] Batch [10/100] D_loss: 0.2065, G_loss: 50.7310, L1: 50.0998
Epoch [1/10] Batch [20/100] D_loss: 0.1418, G_loss: 41.3594, L1: 40.7540
Epoch [1/10] Batch [30/100] D_loss: 0.1189, G_loss: 33.1340, L1: 32.5365
Epoch [1/10] Batch [40/100] D_loss: 0.1202, G_loss: 25.9659, L1: 25.4108
Epoch [1/10] Batch [50/100] D_loss: 0.1384, G_loss: 20.1503, L1: 19.6635
Epoch [1/10] Batch [60/100] D_loss: 0.1636, G_loss: 16.3588, L1: 15.9341
Epoch [1/10] Batch [70/100] D_loss: 0.1775, G_loss: 14.0769, L1: 13.6852
Epoch [1/10] Batch [80/100] D_loss: 0.1826, G_loss: 12.4087, L1: 12.0365
Epoch [1/10] Batch [90/100] D_loss: 0.1874, G_loss: 11.0117, L1: 10.6579
Epoch [2/10] Batch [0/100] D_loss: 0.1962, G_loss: 9.7763, L1: 9.4412
Epoch [2/10] Batch [10/100] D_loss: 0.2087, G_loss: 8.7385, L1: 8.4209
Epoch [2/10] Batch [20/100] D_loss: 0.2226, G_loss: 8.0063, L1: 7.7061
Epoch [2/10] Batch [30/100] D_loss: 0.2344, G_loss: 7.5585, L1: 7.2712
Epoch [2/10] Batch [40/100] D_loss: 0.2419, G_loss: 7.3082, L1: 7.0316
Epoch [2/10] Batch [50/100] D_loss: 0.2459, G_loss: 7.1492, L1: 6.8825
Epoch [2/10] Batch [60/100] D_loss: 0.2480, G_loss: 7.0315, L1: 6.7585
Epoch [2/10] Batch [70/100] D_loss: 0.2487, G_loss: 6.9287, L1: 6.6574
Epoch [2/10] Batch [80/100] D_loss: 0.2484, G_loss: 6.8293, L1: 6.5640
Epoch [2/10] Batch [90/100] D_loss: 0.2483, G_loss: 6.7458, L1: 6.4750
Epoch [3/10] Batch [0/100] D_loss: 0.2481, G_loss: 6.6615, L1: 6.3887
Epoch [3/10] Batch [10/100] D_loss: 0.2478, G_loss: 6.5816, L1: 6.3047
Epoch [3/10] Batch [20/100] D_loss: 0.2477, G_loss: 6.5057, L1: 6.2223
Epoch [3/10] Batch [30/100] D_loss: 0.2466, G_loss: 6.4103, L1: 6.1380
Epoch [3/10] Batch [40/100] D_loss: 0.2461, G_loss: 6.3332, L1: 6.0595
Epoch [3/10] Batch [50/100] D_loss: 0.2463, G_loss: 6.2783, L1: 5.9831
Epoch [3/10] Batch [60/100] D_loss: 0.2450, G_loss: 6.1821, L1: 5.9083
Epoch [3/10] Batch [70/100] D_loss: 0.2445, G_loss: 6.1091, L1: 5.8352
Epoch [3/10] Batch [80/100] D_loss: 0.2443, G_loss: 6.0504, L1: 5.7633
Epoch [3/10] Batch [90/100] D_loss: 0.2437, G_loss: 5.9716, L1: 5.6928
Epoch [4/10] Batch [0/100] D_loss: 0.2431, G_loss: 5.9005, L1: 5.6232
Epoch [4/10] Batch [10/100] D_loss: 0.2434, G_loss: 5.8482, L1: 5.5544
Epoch [4/10] Batch [20/100] D_loss: 0.2424, G_loss: 5.7675, L1: 5.4870
Epoch [4/10] Batch [30/100] D_loss: 0.2421, G_loss: 5.7035, L1: 5.4208
Epoch [4/10] Batch [40/100] D_loss: 0.2422, G_loss: 5.6447, L1: 5.3555
Epoch [4/10] Batch [50/100] D_loss: 0.2416, G_loss: 5.5752, L1: 5.2916
Epoch [4/10] Batch [60/100] D_loss: 0.2414, G_loss: 5.5141, L1: 5.2279
Epoch [4/10] Batch [70/100] D_loss: 0.2412, G_loss: 5.4519, L1: 5.1651
Epoch [4/10] Batch [80/100] D_loss: 0.2408, G_loss: 5.3886, L1: 5.1028
Epoch [4/10] Batch [90/100] D_loss: 0.2406, G_loss: 5.3284, L1: 5.0410
Epoch [5/10] Batch [0/100] D_loss: 0.2404, G_loss: 5.2680, L1: 4.9801
Epoch [5/10] Batch [10/100] D_loss: 0.2400, G_loss: 5.2063, L1: 4.9197
Epoch [5/10] Batch [20/100] D_loss: 0.2398, G_loss: 5.1480, L1: 4.8595
Epoch [5/10] Batch [30/100] D_loss: 0.2395, G_loss: 5.0893, L1: 4.8007
Epoch [5/10] Batch [40/100] D_loss: 0.2393, G_loss: 5.0319, L1: 4.7430
Epoch [5/10] Batch [50/100] D_loss: 0.2390, G_loss: 4.9765, L1: 4.6878
Epoch [5/10] Batch [60/100] D_loss: 0.2388, G_loss: 4.9246, L1: 4.6347
Epoch [5/10] Batch [70/100] D_loss: 0.2386, G_loss: 4.8739, L1: 4.5832
Epoch [5/10] Batch [80/100] D_loss: 0.2383, G_loss: 4.8236, L1: 4.5328
Epoch [5/10] Batch [90/100] D_loss: 0.2381, G_loss: 4.7748, L1: 4.4839
Epoch [6/10] Batch [0/100] D_loss: 0.2379, G_loss: 4.7261, L1: 4.4342
Epoch [6/10] Batch [10/100] D_loss: 0.2377, G_loss: 4.6797, L1: 4.3871
Epoch [6/10] Batch [20/100] D_loss: 0.2374, G_loss: 4.6348, L1: 4.3428
Epoch [6/10] Batch [30/100] D_loss: 0.2372, G_loss: 4.5923, L1: 4.2998
Epoch [6/10] Batch [40/100] D_loss: 0.2370, G_loss: 4.5522, L1: 4.2586
Epoch [6/10] Batch [50/100] D_loss: 0.2368, G_loss: 4.5128, L1: 4.2186
Epoch [6/10] Batch [60/100] D_loss: 0.2365, G_loss: 4.4736, L1: 4.1801
Epoch [6/10] Batch [70/100] D_loss: 0.2364, G_loss: 4.4379, L1: 4.1426
Epoch [6/10] Batch [80/100] D_loss: 0.2362, G_loss: 4.4015, L1: 4.1064
Epoch [6/10] Batch [90/100] D_loss: 0.2360, G_loss: 4.3672, L1: 4.0714
Epoch [7/10] Batch [0/100] D_loss: 0.2357, G_loss: 4.3331, L1: 4.0371
Epoch [7/10] Batch [10/100] D_loss: 0.2355, G_loss: 4.3006, L1: 4.0039
Epoch [7/10] Batch [20/100] D_loss: 0.2353, G_loss: 4.2687, L1: 3.9715
Epoch [7/10] Batch [30/100] D_loss: 0.2351, G_loss: 4.2380, L1: 3.9402
Epoch [7/10] Batch [40/100] D_loss: 0.2349, G_loss: 4.2073, L1: 3.9092
Epoch [7/10] Batch [50/100] D_loss: 0.2347, G_loss: 4.1786, L1: 3.8796
Epoch [7/10] Batch [60/100] D_loss: 0.2344, G_loss: 4.1492, L1: 3.8506
Epoch [7/10] Batch [70/100] D_loss: 0.2342, G_loss: 4.1212, L1: 3.8220
Epoch [7/10] Batch [80/100] D_loss: 0.2340, G_loss: 4.0951, L1: 3.7947
Epoch [7/10] Batch [90/100] D_loss: 0.2338, G_loss: 4.0681, L1: 3.7676
Epoch [8/10] Batch [0/100] D_loss: 0.2336, G_loss: 4.0422, L1: 3.7413
Epoch [8/10] Batch [10/100] D_loss: 0.2334, G_loss: 4.0174, L1: 3.7158
Epoch [8/10] Batch [20/100] D_loss: 0.2332, G_loss: 3.9915, L1: 3.6903
Epoch [8/10] Batch [30/100] D_loss: 0.2330, G_loss: 3.9674, L1: 3.6656
Epoch [8/10] Batch [40/100] D_loss: 0.2327, G_loss: 3.9432, L1: 3.6412
Epoch [8/10] Batch [50/100] D_loss: 0.2325, G_loss: 3.9201, L1: 3.6174
Epoch [8/10] Batch [60/100] D_loss: 0.2323, G_loss: 3.8974, L1: 3.5941
Epoch [8/10] Batch [70/100] D_loss: 0.2321, G_loss: 3.8748, L1: 3.5712
Epoch [8/10] Batch [80/100] D_loss: 0.2319, G_loss: 3.8514, L1: 3.5482
Epoch [8/10] Batch [90/100] D_loss: 0.2314, G_loss: 3.8277, L1: 3.5264
Epoch [9/10] Batch [0/100] D_loss: 0.2311, G_loss: 3.8061, L1: 3.5045
Epoch [9/10] Batch [10/100] D_loss: 0.2315, G_loss: 3.7928, L1: 3.4829
Epoch [9/10] Batch [20/100] D_loss: 0.2311, G_loss: 3.7682, L1: 3.4616
Epoch [9/10] Batch [30/100] D_loss: 0.2307, G_loss: 3.7453, L1: 3.4406
Epoch [9/10] Batch [40/100] D_loss: 0.2307, G_loss: 3.7288, L1: 3.4204
Epoch [9/10] Batch [50/100] D_loss: 0.2305, G_loss: 3.7077, L1: 3.4002
Epoch [9/10] Batch [60/100] D_loss: 0.2303, G_loss: 3.6884, L1: 3.3802
Epoch [9/10] Batch [70/100] D_loss: 0.2302, G_loss: 3.6698, L1: 3.3601
Epoch [9/10] Batch [80/100] D_loss: 0.2298, G_loss: 3.6486, L1: 3.3407
Epoch [9/10] Batch [90/100] D_loss: 0.2297, G_loss: 3.6318, L1: 3.3213
Epoch [10/10] Batch [0/100] D_loss: 0.2295, G_loss: 3.6129, L1: 3.3025
Epoch [10/10] Batch [10/100] D_loss: 0.2293, G_loss: 3.5951, L1: 3.2834
Epoch [10/10] Batch [20/100] D_loss: 0.2291, G_loss: 3.5766, L1: 3.2648
Epoch [10/10] Batch [30/100] D_loss: 0.2288, G_loss: 3.5572, L1: 3.2462
Epoch [10/10] Batch [40/100] D_loss: 0.2287, G_loss: 3.5399, L1: 3.2278
Epoch [10/10] Batch [50/100] D_loss: 0.2285, G_loss: 3.5223, L1: 3.2100
Epoch [10/10] Batch [60/100] D_loss: 0.2283, G_loss: 3.5046, L1: 3.1920
Epoch [10/10] Batch [70/100] D_loss: 0.2281, G_loss: 3.4871, L1: 3.1743
Epoch [10/10] Batch [80/100] D_loss: 0.2280, G_loss: 3.4700, L1: 3.1569
Epoch [10/10] Batch [90/100] D_loss: 0.2277, G_loss: 3.4522, L1: 3.1397
Training finished. Models saved.